{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/afournier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/afournier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/afournier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/afournier/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/afournier/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def process_prompt(prompt):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokens = word_tokenize(prompt)\n",
    "\n",
    "    # Remove stop words and lemmatize with POS tagging\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and token not in stop_words:\n",
    "            pos = get_wordnet_pos(token)\n",
    "            processed_token = lemmatizer.lemmatize(token, pos)\n",
    "            processed_tokens.append(processed_token)\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "def load_and_process_prompts(prompt_file):\n",
    "    with open(prompt_file, 'r') as file:\n",
    "        prompts = json.load(file)\n",
    "    \n",
    "    return {prompt['original_class']: process_prompt(prompt['prompt']) for prompt in prompts}\n",
    "\n",
    "def purge_directory(path):\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def convert_tsv_to_jsonl(input_file, base_output_dir, prompts):\n",
    "    processed_filepaths = set()\n",
    "    max_file_size = 0.5 * 1024 * 1024 \n",
    "\n",
    "    with open(input_file, 'r') as tsv:\n",
    "        for line in tsv:\n",
    "            parts = line.strip().split('\\t')\n",
    "\n",
    "            if len(parts) in [6, 7]:\n",
    "                filepath, event_class, start, end, file_duration, split = parts[:6]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Construct the full path to the audio file\n",
    "            full_file_path = os.path.join('.', filepath)  # Adjust this if the base directory is different\n",
    "\n",
    "            # Check the file size\n",
    "            if os.path.getsize(full_file_path) > max_file_size:\n",
    "                continue  # Skip files larger than 1 MB\n",
    "\n",
    "            if filepath in processed_filepaths:\n",
    "                continue\n",
    "\n",
    "            if event_class.startswith(\"D\") and split in ['train', 'valid', 'test_internal']:\n",
    "                split_dir = {'train': 'train', 'valid': 'validation', 'test_internal': 'test_internal'}[split]\n",
    "                output_dir = os.path.join(base_output_dir, split_dir)\n",
    "                relative_path = filepath\n",
    "                data = {\n",
    "                    \"path\": relative_path,\n",
    "                    \"duration\": float(file_duration),\n",
    "                    \"sample_rate\": 22050,\n",
    "                    \"amplitude\": None,\n",
    "                    \"weight\": None,\n",
    "                    \"info_path\": None\n",
    "                }\n",
    "\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_file = os.path.join(output_dir, 'data.jsonl')\n",
    "                with open(output_file, 'a', encoding='utf-8') as jsonl:\n",
    "                    jsonl.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "                processed_filepaths.add(filepath)\n",
    "\n",
    "                # Create metadata file\n",
    "                metadata_path = os.path.join('metadata', relative_path.replace('.wav', '.json'))\n",
    "                os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n",
    "                description = prompts.get(event_class, \"No description available\")\n",
    "                with open(metadata_path, 'w', encoding='utf-8') as metadata_file:\n",
    "                    json.dump({\"description\": description}, metadata_file, ensure_ascii=False)\n",
    "\n",
    "purge_directory('metadata')\n",
    "purge_directory('./egs/cochldb')\n",
    "\n",
    "# Load prompts and convert TSV to JSONL\n",
    "prompts = load_and_process_prompts('prompts.json')\n",
    "convert_tsv_to_jsonl('data_table.tsv', './egs/cochldb', prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/afournier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/afournier/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/afournier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/afournier/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/afournier/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def process_prompt(prompt):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokens = word_tokenize(prompt)\n",
    "\n",
    "    # Remove stop words and lemmatize with POS tagging\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and token not in stop_words:\n",
    "            pos = get_wordnet_pos(token)\n",
    "            processed_token = lemmatizer.lemmatize(token, pos)\n",
    "            processed_tokens.append(processed_token)\n",
    "\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "def load_and_process_prompts(prompt_file):\n",
    "    with open(prompt_file, 'r') as file:\n",
    "        prompts = json.load(file)\n",
    "    \n",
    "    return {prompt['original_class']: process_prompt(prompt['prompt']) for prompt in prompts}\n",
    "\n",
    "unique_events = set()\n",
    "def convert_tsv_to_jsonl(input_file, base_output_dir, prompts):\n",
    "    processed_filepaths = set()\n",
    "    max_file_size = 0.5 * 1024 * 1024 \n",
    "\n",
    "    with open(input_file, 'r') as tsv:\n",
    "        for line in tsv:\n",
    "            parts = line.strip().split('\\t')\n",
    "\n",
    "            if len(parts) in [6, 7]:\n",
    "                filepath, event_class, start, end, file_duration, split = parts[:6]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Construct the full path to the audio file\n",
    "            full_file_path = os.path.join('.', filepath)  # Adjust this if the base directory is different\n",
    "\n",
    "            # Check the file size\n",
    "            if os.path.getsize(full_file_path) > max_file_size:\n",
    "                continue  # Skip files larger than 1 MB\n",
    "\n",
    "            if filepath in processed_filepaths:\n",
    "                continue\n",
    "\n",
    "            if event_class.startswith(\"D\") and split in ['train']:\n",
    "                unique_events.add(event_class)\n",
    "\n",
    "# Load prompts and convert TSV to JSONL\n",
    "prompts = load_and_process_prompts('prompts.json')\n",
    "convert_tsv_to_jsonl('data_table.tsv', './egs/cochldb', prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dash_camera_sensor',\n",
       " 'Dishes_and_pots_and_pans',\n",
       " 'Dog',\n",
       " 'Dog_bark',\n",
       " 'Dog_growl',\n",
       " 'Dog_howl',\n",
       " 'Dog_whine',\n",
       " 'Door',\n",
       " 'Door_close',\n",
       " 'Door_open',\n",
       " 'Door_open_or_close',\n",
       " 'Door_password',\n",
       " 'Door_slam',\n",
       " 'Doorbell',\n",
       " 'Doorlock_alarm',\n",
       " 'Doorlock_close',\n",
       " 'Doorlock_open',\n",
       " 'Doorlock_open_or_close',\n",
       " 'Double_bass',\n",
       " 'Double_clap',\n",
       " 'Drawer',\n",
       " 'Drawer_open_or_close',\n",
       " 'Drill',\n",
       " 'Drinking',\n",
       " 'Drinking_straw',\n",
       " 'Drip',\n",
       " 'Driving',\n",
       " 'Drum',\n",
       " 'Duck'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
